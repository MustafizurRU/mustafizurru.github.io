<!DOCTYPE html>
<html>
<head>
	<title> Md Mustafizur Rahman - NAIST</title>
	<link rel="icon" type="image/png" href="image/m.png" />
	<meta name="description" content="Md Mustafizur Rahman's Personal Page." />
	<meta http-equiv="author" content="Md Mustafizur Rahman" />
	<link href="css/main.css" type="text/css" rel="stylesheet" />
</head>

<body>

<div id = "navigator">
	<ul>
		<li><a href="#navigator">Profile</a></li>
		<li><a href="#work_experience">Experience</a></li>
		<li><a href="#research">Research</a></li>
		<li><a href="#project">Project</a></li>
		<li><a href="#awards">Awards</a></li>
		<li><a href="#blog">Blog</a></li>
	</ul>
</div>

<div id = "wrapper">

	<!-- profile block -->
	<div id = "profile">
		<div id = "profile_content">
			<div id ="profile_content_name">
				<div id = "profile_content_name_text"> Md Mustafizur Rahman  </div>
<!--				<div id = "profile_content_name_img"><img src="images/name.png", alt="Chinese Name", width="76" height="26"> </div>-->
			</div>

			<div id = "profile_content_description">
			Master's Student <br/>
            <a href="https://imdl.naist.jp/">Interactive Media Design Laboratory</a> <br/>
            <a href="http://isw3.naist.jp/home-en.html">Division of Information Science</a> <br/>
			<a href="http://isw3.naist.jp/home-en.html">Graduate School of Advanced Science and Technology</a> <br/>
			<a href="https://www.naist.jp/en/"> Nara Institute of Science and Technology, Japan</a> <br/>
			<br/>
			Email: rahman.md_mustafizur.rp6@naist.ac.jp <br/>
			<a href="files/Mustafizur_Resume.pdf" target="_blank" class="special_link">Resume</a>
			<a href="https://scholar.google.com/citations?user=YEVQ5b0AAAAJ&hl=en" class="special_link">Google Scholar </a>
			</div>
		</div>

		<div id = "profile_image">
		<img src="image/mustafizur.jpg", class="align-center" alt="Md Mustafizur Rahman", width="120" height="160">
		</div>
	</div>

	<div class = "item_block" id="aboutme">
		<div class="item_headline">
			<h2> About Me </h2>
		</div>
		<div id="aboutme_content">
			<p>I'm a Master's student at the <a href="https://www.naist.jp/en/" class="light_blue_link">Nara Institute of Science and Technology, Japan</a>,
				with a background in Information Science.</p>
			<p>My work focuses on leveraging AI, AR, and VR technologies to enhance physical therapy, medical training, and rehabilitation.</p>
			<p>I have a passion for developing innovative solutions that combine software engineering, computer vision, and interactive media design.</p>
			<p>I love pixels and believe the power of simple intuition and creative thinking. </p>
        </div>
	</div>

	<div class = "item_block" id="work_experience">
		<div class="item_headline">
			<h2> Work Experience </h2>
		</div>

        <table>
			<tr>
				<td class="work_description">
					<a href="https://imdl.naist.jp/" class="light_blue_link" target="_blank">Interactive Media Design Laboratory</a> | Master's Student | Researcher <br>
					October 2023 ~ Present | 8916-5 Takayama-cho, Ikoma, Nara 630-0192, JAPAN <br>
					&bull; Currently engaged in a research project titled "Experience Augmentation in Physical Therapy by Simulating Patient-Specific Walking Motions," utilizing the HumanML3D dataset.<br>
					&bull; Supervised by esteemed lab supervisor <a href="https://scholar.google.com/citations?user=zlyaC60AAAAJ&hl=en" class="light_blue_link" target="_blank">Professor Hirokazu Kato</a> and other Assistant Professor
					<a href="https://scholar.google.com/citations?user=5vnFG2sAAAAJ&hl=ja" class="light_blue_link" target="_blank">Taishi Sawabe</a> and <a href="https://scholar.google.com/citations?user=5qcS7IoAAAAJ&hl=en" class="light_blue_link" target="_blank">Isidro Butaslac</a><br>
					&bull; Focused on developing innovative solutions that enhance physical therapy through the simulation of individualized walking motions.<br>
					&bull; Contributing to cutting-edge research aimed at improving therapeutic outcomes for patients in rehabilitation settings by providing immersive 3D motion simulations.<br>
					&bull; Leveraging expertise in Augmented Reality (AR), Virtual Reality (VR), and Mixed Reality (MR) to create dynamic, patient-specific motion representations.<br>
					&bull; Applying generative AI techniques and large language models (LLMs), including BERT, to analyze and generate impaired human motion data for improved therapeutic applications.<br>
				</td>
				<td class="work_image" id="naist_img">
					<img src="image/imd_lab.png" alt="Interactive Media Design Laboratory" width="140" height="49">
				</td>
			</tr>

			<tr>
				<td class="work_description">
					<a href="https://www.kyoto-u.ac.jp/en" class="light_blue_link" target="_blank">Kyoto University</a> | Research Collaborator <br>
					March 2024 ~ Present | 54 Kawahara-cho, Shogoin, Sakyo-ku, Kyoto 606-8507, JAPAN <br>
					&bull; Collaborating with professors <a href="https://scholar.google.com/citations?user=Oz9_9Z8AAAAJ&hl=en" class="light_blue_link" target="_blank">Goshiro Yamamoto</a>, <a href="https://scholar.google.co.jp/citations?user=jhNjdXcAAAAJ&hl=ja" class="light_blue_link" target="_blank">Chang Liu</a>, and <a href="https://researchmap.jp/hiro-ueshima" class="light_blue_link" target="_blank">Hiroaki Ueshima</a> from the Clinical Research Center for Medical Equipment Development.<br>
					&bull; Engaged in research titled "Experience Augmentation in Physical Therapy by Simulating Patient-Specific Walking Motions," enhancing rehabilitation outcomes through advanced simulation techniques.<br>
					&bull; Involved in multidisciplinary projects focused on optimizing rehabilitation practices in physical therapy settings.<br>
					&bull; Integrating generative AI and LLMs, such as BERT, to improve therapeutic interventions and enhance patient communication.<br>
					&bull; Leveraging Augmented Reality (AR), Virtual Reality (VR), and Mixed Reality (MR) to develop interactive tools that facilitate immersive learning experiences for physical therapists by simulating a wide range of impaired gait patterns.<br>
				</td>
				<td class="work_image" id="kyoto_university_img">
					<img src="image/kyoto_uni.png" alt="Kyoto University" width="140" height="49">
				</td>
			</tr>


			<tr>
                <td class="work_description">
                    <a href="https://talentpro.global/" target="_blank" class="light_blue_link">Talent Pro</a> | Team Lead - Software Quality Assurance Engineer <br>
                    June 2022 ~ August 2023 | 109 Masjid Road, Old DOHS, Banani, Dhaka 1206, Bangladesh. <br>
                    &bull; Led the QA efforts and managed testing processes for various projects at TalentPro. <br>
                    &bull; Created and executed test plans, test cases, and designed automation test scripts.<br>
                    &bull; Conducted test execution result analysis. <br>
                    &bull; Specialized in Appium, Selenium WebDriver, TestNG, and Cucumber within Java-based automation frameworks (TDD, BDD). <br>
                    &bull; Managed API testing, performance testing, security testing, and database testing using REST Assured and GraphQL.
                </td>
                <td class="work_image" id="talentpro_img">
                    <img src="image/talentpro-global-logo.jpg" alt="Talent Pro" width="140" height="49">
                </td>
            </tr>

            <tr>
                <td class="work_description">
                    <a href="https://realezy.com/" target="_blank" class="light_blue_link">RealEzy (Singapore)</a> | Software Quality Assurance Engineer <br>
                    June 2022 ~ August 2023 | Singapore-based project under TalentPro <br>
                    &bull; Hired by RealEzy, a leading Singapore real estate platform, for a dedicated QA role on their project. <br>
                    &bull; Responsible for automating test processes, designing test plans, and ensuring software quality through manual and automated testing. <br>
                    &bull; Worked extensively with Appium, Selenium, and Java-based automation frameworks to streamline testing efforts for RealEzy's platform. <br>
                    &bull; Performed API, performance, and security testing using REST Assured, ensuring optimal functionality for the platform.
                </td>
                <td class="work_image" id="realezy_img">
                    <img src="image/realEzy_Logo.svg" alt="RealEzy" width="140" height="49">
                </td>
            </tr>

            <tr>
                <td class="work_description">
                    <a href="https://fanfare.com.bd/" target="_blank" class="light_blue_link">Fanfare (Bangladesh)</a> | Team Lead - Software Quality Assurance Engineer <br>
                    March 2023 ~ May 2023 | Bangladesh-based project under TalentPro <br>
                    &bull; Assigned to Fanfare, a social commerce platform, to ensure quality in their software releases for three months. <br>
                    &bull; Developed and executed test plans, test cases, and automated testing scripts to support the platform's quality assurance. <br>
                    &bull; Utilized Appium, Selenium, and Java-based automation frameworks to optimize test cycles. <br>
                    &bull; Conducted API and performance testing using REST Assured, ensuring smooth integration of new features and updates.
                </td>
                <td class="work_image" id="fanfare_img">
                    <img src="image/fanfarelogo.png" alt="Fanfare" width="140" height="49">
                </td>
            </tr>
        </table>
	</div>

	<!--
	<div class = "item_block" id="news">
		<div class="item_headline">
			<h2> News </h2>
		</div>

		<a href="https://www.technologyreview.com/s/612647/machine-vision-creates-harry-potter-style-magic-photos/"> <img alt="" src="./images/press/mit.png" style="width:17%; margin:10px"></a>
		<a href="https://www.washington.edu/news/2019/06/11/making-moving-photos-a-reality/"> <img alt="" src="./images/press/uwnews.png" style="width:10%; margin:8px"></a>
		<a href="https://news.developer.nvidia.com/transforming-paintings-and-photos-into-animations-with-ai/"> <img alt="" src="./images/press/nvidia.png" style="width:12%; margin:10px 20px"></a>
		<a href="https://www.digitaltrends.com/cool-tech/ar-figures-walk-off-backdrop/"> <img alt="" src="./images/press/dt.jpg" style="width:10%; margin:8px"></a>
		<a href="https://www.geekwire.com/2019/photo-wake-makes-still-photographs-picasso-paintings-come-spookily-alive/"> <img alt="" src="./images/press/geekwire.png" style="width:10%; margin:10px 20px"></a>
		<a href="https://next.reality.news/news/university-washington-researchers-demo-ability-generate-3d-augmented-reality-content-from-2d-images-0191910/"> <img alt="" src="./images/press/next-reality.png" style="width:13%; margin:10px"></a>
		<a href="https://petapixel.com/2019/06/17/this-ai-can-bring-a-person-to-life-from-a-single-still-photo/"> <img alt="" src="./images/press/petapixel.png" style="width:15%; margin:10px"></a>
		<a href="https://gizmodo.com/a-new-ai-powered-system-creates-impressive-3d-animation-1831259377"> <img alt="" src="./images/press/gizmodo.png" style="width:12%; margin:10px"></a>
		<a href="https://vrscout.com/news/photo-wake-up-ar-animations/"> <img alt="" src="./images/press/vrscout.png" style="width:20%; margin:10px"></a>
		<a href="https://futurism.com/algorithm-3d-animations-still-image"> <img alt="" src="./images/press/futurism.png" style="width:8%; margin:10px"></a>
		<a href="https://venturebeat.com/2019/02/26/ar-and-vr-creators-have-an-unheralded-tool-to-make-their-content-shine-3d-reconstruction-tech/"> <img alt="" src="./images/press/venture-beat.png" style="width:20%; margin:5px"></a>
		<a href="https://tw.appledaily.com/international/20190104/JQOMWPQJ6DJXDWZO64U5OT5QTE/"> <img alt="" src="./images/press/apple_news.png" style="width:8%; margin:10px"></a>
	</div>
	-->


	<div class = "item_block" id="research">
		<div class="item_headline">
			<h2>Research </h2>
		</div>

		<!-- Specific-Motion -->
		<div class="project_item">
			<table>
				<tbody>
				<tr><td class="project_image"><img src="image/research-img/1TeaserImage.png" class="align-center" alt="Specific Walking Motion " width="800" height="236" /></td></tr>
				<tr><td class="project_title"><a href="#" class="project_title_link">Experience Augmentation in Physical Therapy by
					Simulating Patient-Specific Walking Motions</a></td></tr>
				<tr><td class="project_author">
					<a href="https://scholar.google.com/citations?user=YEVQ5b0AAAAJ&hl=en" class="light_blue_link" target="_blank"><b>Md Mustafizur Rahman</b></a>,
					<a href="https://scholar.google.com/citations?user=Oz9_9Z8AAAAJ&hl=en" class="light_blue_link" target="_blank">Goshiro Yamamoto</a>,
					<a href="https://scholar.google.co.jp/citations?user=jhNjdXcAAAAJ&hl=ja" class="light_blue_link" target="_blank">Chang Liu</a>,
					<a href="https://researchmap.jp/hiro-ueshima" class="light_blue_link" target="_blank">Hiroaki Ueshima</a>,
					<a href="https://scholar.google.com/citations?user=5qcS7IoAAAAJ&hl=en" class="light_blue_link" target="_blank">Isidro Butaslac</a>,
					<a href="https://scholar.google.com/citations?user=5vnFG2sAAAAJ&hl=ja" class="light_blue_link" target="_blank">Taishi Sawabe</a>,
					<a href="https://scholar.google.com/citations?user=zlyaC60AAAAJ&hl=en" class="light_blue_link" target="_blank">Hirokazu Kato</a>
				</td><tr>
<!--				<tr><td class="research_misc"><i>arXiv,</i> 2020</td></tr>-->
				<tr><td class="project_description">
					Given a dataset of motion captures, this system allows physical therapists to learn from diverse impaired gait patterns without constraints of time or patient availability. Utilizing the HumanML3D dataset, it combines text2length sampling for predicting motion length and a temporal variational autoencoder for generating consistent 3D motion sequences. The system provides immersive, patient-specific motion simulations in mixed reality, enhancing therapeutic training and strategies.
<!--					<p>Here we rebuild a 3D animatable Roger Federer from a video of 2015 US Open Final. Please check out the dynamic versions of the results on the <a href="https://grail.cs.washington.edu/projects/vid2actor/" class="light_blue_link">project page</a>.</p>-->
				</td></tr>
				<tr><td class="project_appendix">
					<a href="#" class="project_link">project page</a> |
					<a href="#" class="project_link">paper</a> |
					<a href="#" class="project_link">video</a>
				</tr></td>
				</tbody>
			</table>
			<div class="back_to_top"><a href="#navigator" class="black_link">back to top</a></div>
		</div>


<!--        &lt;!&ndash; HumanNeRF &ndash;&gt;-->
<!--		<div class="project_item">-->
<!--			<table>-->
<!--				<tbody>-->
<!--					<tr><td class="project_image">-->
<!--                        <img src="images\humannerf_image.jpg" class="align-left" style="vertical-align: bottom; padding-bottom: 48px" alt="humannerf" width="340" />-->
<!--                        <video width="55%" autoplay loop muted>-->
<!--                            <source src="images/humannerf_video.mp4" type="video/mp4">-->
<!--                        </video>-->
<!--                    </td></tr>-->
<!--					<tr><td class="project_title"><a href="https://grail.cs.washington.edu/projects/humannerf/" class="project_title_link">HumanNeRF: Free-viewpoint Rendering of Moving People from Monocular Video</a></td></tr>-->
<!--					<tr><td class="project_author">-->
<!--						<b>Chung-Yi Weng</b>,-->
<!--						<a href="https://homes.cs.washington.edu/~curless/" class="author_link">Brian Curless</a>,-->
<!--                        <a href="https://pratulsrinivasan.github.io/" class="author_link">Pratul P. Srinivasan</a>,-->
<!--                        <a href="https://jonbarron.info/" class="author_link">Jonathan T. Barron</a>,-->
<!--						<a href="https://homes.cs.washington.edu/~kemelmi/" class="author_link">Ira Kemelmacher-Shlizerman</a>-->
<!--					</td><tr>-->
<!--					<tr><td class="research_misc"><i>CVPR,</i> 2022 <b><font color="red">(Oral Presentation)</font></b></td></tr>-->
<!--					<tr><td class="project_description">-->
<!--						We introduce a free-viewpoint rendering method &#45;&#45; HumanNeRF &#45;&#45; that works on a given monocular video of a human performing complex body motions, e.g. a video from YouTube. Our method enables pausing the video at any frame and rendering the subject from arbitrary new camera viewpoints or even a full 360-degree camera path for that particular frame and body pose.-->
<!--					</td></tr>-->
<!--					<tr><td class="project_appendix">-->
<!--						<a href="https://grail.cs.washington.edu/projects/humannerf/" class="project_link">project page</a> |-->
<!--						<a href="https://arxiv.org/abs/2201.04127" class="project_link">paper</a> |-->
<!--                        <a href="https://youtu.be/GM-RoZEymmw" class="project_link">video</a> |-->
<!--                        <a href="https://github.com/chungyiweng/humannerf" class="project_link">code</a>-->
<!--					</tr></td>-->
<!--				</tbody>-->
<!--			</table>-->
<!--			<div class="back_to_top"><a href="#navigator" class="black_link">back to top</a></div>-->
<!--		</div>-->

<!--		&lt;!&ndash; Vid2Actor &ndash;&gt;-->
<!--		<div class="project_item">-->
<!--			<table>-->
<!--				<tbody>-->
<!--					<tr><td class="project_image"><img src="images\vid2actor.png" class="align-center" alt="photo-wakeup" width="800" height="236" /></td></tr>-->
<!--					<tr><td class="project_title"><a href="https://grail.cs.washington.edu/projects/vid2actor/" class="project_title_link">Vid2Actor: Free-viewpoint Animatable Person Synthesis from Video in the Wild</a></td></tr>-->
<!--					<tr><td class="project_author">-->
<!--						<b>Chung-Yi Weng</b>,-->
<!--						<a href="https://homes.cs.washington.edu/~curless/" class="author_link">Brian Curless</a>,-->
<!--						<a href="https://homes.cs.washington.edu/~kemelmi/" class="author_link">Ira Kemelmacher-Shlizerman</a>-->
<!--					</td><tr>-->
<!--					<tr><td class="research_misc"><i>arXiv,</i> 2020</td></tr>-->
<!--					<tr><td class="project_description">-->
<!--						Given an "in-the-wild" video, we train a deep network with the video frames to produce an animatable human representation that can be rendered from any camera view in any body pose, enabling applications such as motion re-targeting and bullet-time rendering without the need for rigged 3D meshes.-->
<!--						<p>Here we rebuild a 3D animatable Roger Federer from a video of 2015 US Open Final. Please check out the dynamic versions of the results on the <a href="https://grail.cs.washington.edu/projects/vid2actor/" class="light_blue_link">project page</a>.</p>-->
<!--					</td></tr>-->
<!--					<tr><td class="project_appendix">-->
<!--						<a href="https://grail.cs.washington.edu/projects/vid2actor/" class="project_link">project page</a> |-->
<!--						<a href="https://arxiv.org/abs/2012.12884" class="project_link">paper</a> |-->
<!--                        <a href="https://youtu.be/Zec8Us0v23o" class="project_link">video</a>-->
<!--					</tr></td>-->
<!--				</tbody>-->
<!--			</table>-->
<!--			<div class="back_to_top"><a href="#navigator" class="black_link">back to top</a></div>-->
<!--		</div>-->

<!--		&lt;!&ndash; Photo Wake-up&ndash;&gt;-->
<!--		<div class="project_item">-->
<!--			<table>-->
<!--				<tbody>-->
<!--					<tr><td class="project_image"><img src="images\photo-wakeup.png" class="align-center" alt="photo-wakeup" width="800" height="288" /></td></tr>-->
<!--					<tr><td class="project_title"><a href="https://grail.cs.washington.edu/projects/wakeup/" class="project_title_link">Photo Wake-Up: 3D Character Animation from a Single Photo</a></td></tr>-->
<!--					<tr><td class="project_author">-->
<!--						<b>Chung-Yi Weng</b>,-->
<!--						<a href="https://homes.cs.washington.edu/~curless/" class="author_link">Brian Curless</a>,-->
<!--						<a href="https://homes.cs.washington.edu/~kemelmi/" class="author_link">Ira Kemelmacher-Shlizerman</a>-->
<!--					</td><tr>-->
<!--					<tr><td class="research_misc"><i>CVPR,</i> 2019</td></tr>-->
<!--					<tr><td class="project_description">-->
<!--						Given a single photo as input (far left), we create a 3D animatable version of the subject, which can now walk towards the viewer (middle).-->
<!--						The 3D result can be experienced in augmented reality (right); in the result above the user has virtually hung the artwork with a HoloLens headset and can watch the character run out of the painting from different views.-->
<!--						Please get details and dynamic versions of the results on the <a href="https://grail.cs.washington.edu/projects/wakeup/" class="light_blue_link">project page</a>.-->
<!--					</td></tr>-->
<!--					<tr><td class="project_appendix">-->
<!--						<a href="https://grail.cs.washington.edu/projects/wakeup/" class="project_link">project page</a> |-->
<!--						<a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Weng_Photo_Wake-Up_3D_Character_Animation_From_a_Single_Photo_CVPR_2019_paper.pdf" class="project_link">paper</a> |-->
<!--                        <a href="https://youtu.be/G63goXc5MyU" class="project_link">video</a> |-->
<!--						<a href="https://www.technologyreview.com/s/612647/machine-vision-creates-harry-potter-style-magic-photos/" class="project_link">MIT Tech Review</a>-->
<!--					</tr></td>-->
<!--				</tbody>-->
<!--			</table>-->
<!--			<div class="back_to_top"><a href="#navigator" class="black_link">back to top</a></div>-->
<!--		</div>-->

<!--		&lt;!&ndash; RoletNet &ndash;&gt;-->
<!--		<div class="project_item">-->
<!--			<table>-->
<!--				<tbody>-->
<!--					<tr><td class="project_image"><img src="images\rolenet.png" class="align-center" alt="RoleNet" width="776" height="287" /></td></tr>-->
<!--					<tr><td class="project_title"><a href="files/RoleNet_MovieAnalysisFromThePerspectiveOfSocialNetworks.pdf" class="project_title_link">RoleNet: Movie Analysis from the Perspective of Social Networks</a></td></tr>-->
<!--					<tr><td class="project_author">-->
<!--						<b>Chung-Yi Weng</b>,-->
<!--						<a href="http://mmcv.csie.ncku.edu.tw/~wtchu/" class="author_link">Wei-Ta Chu</a>,-->
<!--						<a href="https://www.cmlab.csie.ntu.edu.tw/cml/dsp/prof-wu/prof-wu.html" class="author_link">Ja-Ling Wu-->
<!--					</td><tr>-->
<!--					<tr><td class="research_misc"><i>IEEE Transactions on Multimedia</i> 2009</td></tr>-->
<!--					<tr><td class="project_description">Inspired by idea of social network analysis, we propose a novel way to analyze movie videos from the perspective of social networks. The relationship between characters in a movie is elaborately described as a network, called RoleNet. Based on RoletNet, further network analysis is performed to extract semantic information in the movie, including leading roles, macro/micro community structures, and hidden story lines and story units.</td></tr>-->
<!--					<tr><td class="project_appendix"><a href="http://mmcv.csie.ncku.edu.tw/~wtchu/papers/2009IEEEMM-weng.pdf" class="project_link">paper</a></tr></td>-->
<!--				</tbody>-->
<!--			</table>-->
<!--			<div class="back_to_top"><a href="#navigator" class="black_link">back to top</a></div>-->
<!--		</div>-->

<!--		&lt;!&ndash; Tiling Slidshow &ndash;&gt;-->
<!--		<div class="project_item">-->
<!--			<table>-->
<!--				<tbody>-->
<!--					<tr><td class="project_image"><img src="images\TilingSlideshow.jpg" class="align-center" alt="Tiling Slideshow" width="722" height="425" /></td></tr>-->
<!--					<tr><td class="project_title"><a href="https://www.cmlab.csie.ntu.edu.tw/TilingSlideshow/" class="project_title_link">Tiling Slideshow</a></td></tr>-->
<!--					<tr><td class="project_author">-->
<!--						<a href="https://scholar.google.com.au/citations?user=3x9KITUAAAAJ&hl=en" class="author_link">Jun-Cheng Chen</a>,-->
<!--						<a href="http://mmcv.csie.ncku.edu.tw/~wtchu/" class="author_link">Wei-Ta Chu</a>,-->
<!--						<a href="https://www.researchgate.net/scientific-contributions/Jin-Hau-Kuo-9576006" class="author_link">Jin-Hau Kuo</a>,-->
<!--						<b>Chung-Yi Weng</b>,-->
<!--						<a href="https://www.cmlab.csie.ntu.edu.tw/cml/dsp/prof-wu/prof-wu.html" class="author_link">Ja-Ling Wu</a>-->
<!--					</td><tr>-->
<!--					<tr><td class="research_misc"><i>ACM Multimedia</i> 2006 <b><font color="red">(Best Paper Award)</font></b> </td></tr>-->
<!--					<tr><td class="project_description">Tiling Slideshow is a brave new photo displaying method to arrange photos in a tile-like manner, coordinating with the pace of background music. Photo clustering is applied based on the relationship between photos; music beat detection is perfomed in order to trigger the progress of slideshow; photo importance is computed to help ROI determination. Finally, the layout organization is formulated as a constrianed optimization problem to make sure the most satisfied composition results could be produced.</td></tr>-->
<!--					<tr><td class="project_appendix">-->
<!--						<a href="https://www.cmlab.csie.ntu.edu.tw/TilingSlideshow/" class="project_link">project page</a> |-->
<!--						<a href="https://www.cmlab.csie.ntu.edu.tw/new_cml_website/media/publications/Chen-2006-TS.pdf" class="project_link">paper</a>-->
<!--					</tr></td>-->
<!--				</tbody>-->
<!--			</table>-->
<!--			<div class="back_to_top"><a href="#navigator" class="black_link">back to top</a></div>-->
<!--		</div>-->
	</div>

<!--	<div class = "item_block" id="project">-->
<!--		<div class="item_headline">-->
<!--			<h2>Project </h2>-->
<!--		</div>-->

<!--		&lt;!&ndash; Moonriver &ndash;&gt;-->
<!--		<div class="project_item">-->
<!--			<table>-->
<!--				<tbody>-->
<!--					<tr>-->
<!--						<td class="project_image"><img src="images\digitspace.gif" align="left" alt="Digit Space" width="427" height="429" />-->
<!--												  <img src="images\moonriver.png" align="right" alt="MoonRiver" width="332" height="429" />-->
<!--						</td>-->
<!--					</tr>-->
<!--					<tr><td class="project_title"><a href="files/moonriver_report.pdf" class="project_title_link">MoonRiver: Deep Neural Network in C++</a></td></tr>-->
<!--					<tr><td class="project_author">-->
<!--						<b>Chung-Yi Weng</b>-->
<!--					</td><tr>-->
<!--                    <tr><td class="project_description">-->
<!--                        <p>MoonRiver is a deep neural network framework built from scratch using C++.-->
<!--                           Our goal is to shed light on the complex inner working flow of network learning.-->
<!--                           We have designed MoonRiver to be-->
<!--                           (1) <i>lightweight</i>: no third-party dependencies, easy to compile with any standard C++ compiler;-->
<!--                           (2) <i>scalable</i>: effortlessly designing and learning large networks with minimal fuss. </p>-->
<!--                        <p>We demonstrate the effectiveness of MoonRiver by training and testing auto-encoder and LeNet.-->
<!--                        The codes used to implement these networks are concise and the experimental results are promising.</p>-->
<!--                    </td></tr>-->
<!--					<tr><td class="project_appendix">-->
<!--						<a href="projects/moonriver/poster.pdf" class="project_link">poster</a> |-->
<!--						<a href="projects/moonriver/report.pdf" class="project_link">report</a>-->
<!--					</tr></td>-->
<!--				</tbody>-->
<!--			</table>-->
<!--			<div class="back_to_top"><a href="#navigator" class="black_link">back to top</a></div>-->
<!--		</div>-->

<!--		&lt;!&ndash; Become Brad Pitt &ndash;&gt;-->
<!--		<div class="project_item" id="become_brad_pitt">-->
<!--			<table>-->
<!--				<tbody>-->
<!--					<tr><td class="project_image"><img src="images\become_brad_pitt.jpg" class="align-center" alt="become brad pitt" width="700" height="259" /></td></tr>-->
<!--					<tr><td class="project_title"><a href="files/chungyi_2016_vision_project.pdf" class="project_title_link">Becoming Brad Pitt</a></td></tr>-->
<!--					<tr><td class="project_author">-->
<!--						<b>Chung-Yi Weng</b>,-->
<!--                        <a href="https://roxanneluo.github.io/", class="author_link">Xuan Luo</a>-->
<!--					</td><tr>-->
<!--                    <tr><td class="course_project_misc">Final Project, Computer Vision (UW CSE576)</td></tr>-->
<!--					<tr><td class="project_description">-->
<!--                        In this project, we designed a face reenactment system that enables users to control another person's (e.g Brad Pitt) head motions and facial expressions.-->
<!--                        Our system involves implementing a high-speed tracking module, a puppetry module to control facial expression and a morphig module to enforce smooth transition when switching inbween different identities.-->
<!--                        Experiments show that our system can reenact head motions and facial expressions of a target person in real time.-->
<!--                    </td></tr>-->
<!--					<tr><td class="project_appendix">-->
<!--						<a href="projects/bradpitt/report.pdf" class="project_link">report</a> |-->
<!--						<a href="https://drive.google.com/file/d/1ygbj2-g9c41DgjLdS79YwIFLpCSQyCIR/view?usp=sharing" class="project_link">video</a>-->
<!--					</tr></td>-->
<!--				</tbody>-->
<!--			</table>-->
<!--			<div class="back_to_top"><a href="#navigator" class="black_link">back to top</a></div>-->
<!--		</div>-->

<!--		&lt;!&ndash; Heyperlapse &ndash;&gt;-->
<!--		<div class="project_item" id="hyperlapse">-->
<!--			<table>-->
<!--				<tbody>-->
<!--					<tr><td class="project_image"><img src="images\hyperlapse.jpg" class="align-center" alt="hyperlapse" width="632" height="341" /></td></tr>-->
<!--					<tr><td class="project_title"><a href="projects/hyperlapse/index.html" class="project_title_link">Hyperlapse Video Creation</a></td></tr>-->
<!--					<tr><td class="project_author">-->
<!--						<b>Chung-Yi Weng</b>,-->
<!--                        <a href="https://yangxinuw.github.io/", class="author_link">Xin Yang</a>-->
<!--					</td><tr>-->
<!--                    <tr><td class="course_project_misc">Final Project, Computer Graphics (UW CSE557)</td></tr>-->
<!--					<tr><td class="project_description">-->
<!--                        The project aims to create a hyperlapse video in real-time.-->
<!--                        To this end, we describe the input video as a graph where the nodes represent frames and the edges are cost functions that penalize undesired frame transitions.-->
<!--                        A heyperlapse video with a given target speed can be generated by finding an optimal path in the graph.-->
<!--                    </td></tr>-->
<!--					<tr><td class="project_appendix">-->
<!--						<a href="projects/hyperlapse/index.html" class="project_link">project page</a>-->
<!--					</tr></td>-->
<!--				</tbody>-->
<!--			</table>-->
<!--			<div class="back_to_top"><a href="#navigator" class="black_link">back to top</a></div>-->
<!--		</div>-->

<!--        &lt;!&ndash; Local Collision Avoidance for a Nano-drone &ndash;&gt;-->
<!--		<div class="project_item" id="local_collision">-->
<!--			<table>-->
<!--				<tbody>-->
<!--					<tr><td class="project_image"><img src="projects/robotics/teaser.jpg" class="align-center" alt="nano-drone" width="632" height="341" /></td></tr>-->
<!--					<tr><td class="project_title"><a href="projects/robotics/video.mp4" class="project_title_link">Local Collision Avoidance for a Nano-drone</a></td></tr>-->
<!--					<tr><td class="project_author">-->
<!--						<a href="https://www.youtube.com/watch?v=8SGx2qmo9M4" class="author_link">Melanie Anderson,  </a>-->
<!--                        <b>Chung-Yi Weng</b>,-->
<!--                        <a href="https://bindita.github.io/" class="author_link">Bindita Chaudhuri</a>-->
<!--					</td><tr>-->
<!--                    <tr><td class="course_project_misc">Final Project, Robotics (UW CSE571)</td></tr>-->
<!--					<tr><td class="project_description">-->
<!--                        In this project, we have come up with a system that lets a nano-drone navigate an indoor space filled with static and moving objects.-->
<!--                        We do this by creating a map of the environment using raw laser data, represented as an occupancy grid.-->
<!--                        To find the best route to the goal, we use A* search.-->
<!--                        If any moving objects get in the way during navigation, we update the map and come up with a new obstacle-free path.-->
<!--                    </td></tr>-->
<!--					<tr><td class="project_appendix">-->
<!--						<a href="projects/robotics/report.pdf" class="project_link">report</a>-->
<!--                        <a href="projects/robotics/video.mp4" class="project_link">video</a>-->
<!--					</tr></td>-->
<!--				</tbody>-->
<!--			</table>-->
<!--			<div class="back_to_top"><a href="#navigator" class="black_link">back to top</a></div>-->
<!--		</div>-->

<!--		<div class = "item_block" id="industry_product">-->
<!--            <div class="item_headline">-->
<!--                <h2>Industry Product</h2>-->
<!--            </div>-->

<!--		&lt;!&ndash; magic selection &ndash;&gt;-->
<!--		<div class="project_item" id="magic_selection">-->
<!--			<table>-->
<!--				<tbody>-->
<!--					<tr><td class="project_image"><img src="images\MagicSelection.jpg" class="align-center" alt="Magic Selection" width="784" height="198" /></td></tr>-->
<!--					<tr><td class="project_title">Magic Selection</td></tr>-->
<!--					<tr><td class="project_misc">Industry Product @ CyberLink</td></tr>-->
<!--					<tr><td class="project_description">-->
<!--                        Magic Selection assists users in selecting objects on an image by predicting object boundaries based on user-drawn foreground/background strokes.-->
<!--                    </td></tr>-->
<!--				</tbody>-->
<!--			</table>-->
<!--			<div class="back_to_top"><a href="#navigator" class="black_link">back to top</a></div>-->
<!--		</div>-->

<!--		&lt;!&ndash; smart lasso &ndash;&gt;-->
<!--		<div class="project_item" id="smart_lasso">-->
<!--			<table>-->
<!--				<tbody>-->
<!--					<tr><td class="project_image"><img src="images\SmartLasso.jpg" class="align-center" alt="Smart Lasso" width="784" height="197" /></td></tr>-->
<!--					<tr><td class="project_title">Smart Lasso</td></tr>-->
<!--					<tr><td class="project_misc">Industry Product @ CyberLink</td></tr>-->
<!--					<tr><td class="project_description">-->
<!--                        Smart Lasso adjusts a user-drawn boundary to fit the true object boundary, providing an alternative approach for labeling objects.</td></tr>-->
<!--				</tbody>-->
<!--			</table>-->
<!--			<div class="back_to_top"><a href="#navigator" class="black_link">back to top</a></div>-->
<!--		</div>-->

<!--		&lt;!&ndash; soft matting &ndash;&gt;-->
<!--		<div class="project_item" id="soft_matting">-->
<!--			<table>-->
<!--				<tbody>-->
<!--					<tr><td class="project_image"><img src="images\SoftMatting.jpg" class="align-center" alt="Soft Matting" width="784" height="225" /></td></tr>-->
<!--					<tr><td class="project_title">Soft Matting</td></tr>-->
<!--					<tr><td class="project_misc">Industry Product @ CyberLink</td></tr>-->
<!--					<tr><td class="project_description">-->
<!--                        Soft Matting predicts opacity values on user-labeled regions, applying image matting in an interactive manner.-->
<!--                        Matting technique is helpful for refining boundary of objects such as fur or hairs.-->
<!--                    </td></tr>-->
<!--				</tbody>-->
<!--			</table>-->
<!--			<div class="back_to_top"><a href="#navigator" class="black_link">back to top</a></div>-->
<!--		</div>-->

<!--		&lt;!&ndash; Perfect Group Shot &ndash;&gt;-->
<!--		<div class="project_item" id="perfect_group_shot">-->
<!--			<table>-->
<!--				<tbody>-->
<!--					<tr><td class="project_image"><img src="images\PerfectGroupShot.jpg" class="align-center" alt="Perfect Group Shot" width="784" height="215" /></td></tr>-->
<!--					<tr><td class="project_title">Perfect Group Shot</td></tr>-->
<!--					<tr><td class="project_misc">Industry Product @ CyberLink</td></tr>-->
<!--					<tr><td class="project_description">-->
<!--                        By capturing several photos in a short period, Perfect Group Shot creates a group photo by seamlessly compositing human subjects with the best facial expressions (e.g. smiling without eye blinking) from different images.-->
<!--                    </td></tr>-->
<!--				</tbody>-->
<!--			</table>-->
<!--			<div class="back_to_top"><a href="#navigator" class="black_link">back to top</a></div>-->
<!--		</div>-->

<!--		&lt;!&ndash; Sequence Shot &ndash;&gt;-->
<!--		<div class="project_item" id="sequence_shot">-->
<!--			<table>-->
<!--				<tbody>-->
<!--					<tr><td class="project_image"><img src="images\SequenceShot.jpg" class="align-center" alt="Sequence Shot" width="784" height="216" /></td></tr>-->
<!--					<tr><td class="project_title">Sequence Shot</td></tr>-->
<!--					<tr><td class="project_misc">Industry Product @ CyberLink</td></tr>-->
<!--					<tr><td class="project_description">-->
<!--                        Sequence Shot creates a time-lapse effect in a single image by detecting and combining moving subjects from multiple photos.-->
<!--                    </td></tr>-->
<!--				</tbody>-->
<!--			</table>-->
<!--			<div class="back_to_top"><a href="#navigator" class="black_link">back to top</a></div>-->
<!--		</div>-->

<!--		&lt;!&ndash; Panorama &ndash;&gt;-->
<!--		<div class="project_item" id="panorama">-->
<!--			<table>-->
<!--				<tbody>-->
<!--					<tr><td class="project_image"><img src="images\Panorama.jpg" class="align-center" alt="Panorama" width="784" height="256" /></td></tr>-->
<!--					<tr><td class="project_title">Panorama</td></tr>-->
<!--					<tr><td class="project_misc">Industry Product @ CyberLink</td></tr>-->
<!--					<tr><td class="project_description">-->
<!--                        Panorama is an image stiching technique for creating a wide-angle view photo from multiple images.-->
<!--                        Our system allows for taking as input out-of-order images and is robust to panning, zooming, and moving objects.-->
<!--                    </td></tr>-->
<!--				</tbody>-->
<!--			</table>-->
<!--			<div class="back_to_top"><a href="#navigator" class="black_link">back to top</a></div>-->
<!--		</div>-->

<!--		&lt;!&ndash; Holmes &ndash;&gt;-->
<!--		<div class="project_item" id="holmes">-->
<!--			<table>-->
<!--				<tbody>-->
<!--					<tr><td class="project_image"><img src="images\Holmes.jpg" class="align-center" alt="Holmes" width="784" height="282" /></td></tr>-->
<!--					<tr><td class="project_title">Holmes &dash; (Object Tracking by Rectangle or Point)</td></tr>-->
<!--					<tr><td class="project_misc">Industry Product @ CyberLink</td></tr>-->
<!--					<tr><td class="project_description">-->
<!--                        Holmes tracks objects in a video given a point or a rectangle as input. The tracked point or rectangle are used for video editing tasks such as adding a dialog box on top of an object.-->
<!--                    </td></tr>-->
<!--				</tbody>-->
<!--			</table>-->
<!--			<div class="back_to_top"><a href="#navigator" class="black_link">back to top</a></div>-->
<!--		</div>-->

<!--		&lt;!&ndash; Cupid &ndash;&gt;-->
<!--		<div class="project_item" id="cupid">-->
<!--			<table>-->
<!--				<tbody>-->
<!--					<tr><td class="project_image"><img src="images\Cupid.jpg" class="align-center" alt="Cupid" width="784" height="158" /></td></tr>-->
<!--					<tr><td class="project_title">Cupid &dash; (Object Tracking by Object Boundary)</td></tr>-->
<!--					<tr><td class="project_misc">Industry Product @ CyberLink</td></tr>-->
<!--					<tr><td class="project_description">-->
<!--                        Cupid tracks an object as well as its region in a video, taken as input a user-labeled object mask at the first frame.-->
<!--                        The generated object masks are used for downstream video editing such as applying filters on the labeled object.-->
<!--                    </td></tr>-->
<!--				</tbody>-->
<!--			</table>-->
<!--			<div class="back_to_top"><a href="#navigator" class="black_link">back to top</a></div>-->
<!--		</div>-->

<!--		&lt;!&ndash; Robust Face Detection &ndash;&gt;-->
<!--		<div class="project_item" id="robust_face_detection">-->
<!--			<table>-->
<!--				<tbody>-->
<!--					<tr><td class="project_image"><img src="images\RobustFaceDetection.jpg" class="align-center" alt="Robust Face Detection" width="784" height="245" /></td></tr>-->
<!--					<tr><td class="project_title">Robust Face Detection</td></tr>-->
<!--					<tr><td class="project_misc">Industry Product @ CyberLink</td></tr>-->
<!--					<tr><td class="project_description">-->
<!--                        Building upon sparse and high-dimension features, we train a face detector with a hierarchical architecture that performs swift detection and is robust to in-plane and out-of-plane rotations.-->
<!--                    </td></tr>-->
<!--				</tbody>-->
<!--			</table>-->
<!--			<div class="back_to_top"><a href="#navigator" class="black_link">back to top</a></div>-->
<!--		</div>-->

<!--		&lt;!&ndash; Facial Landmark Localization &ndash;&gt;-->
<!--		<div class="project_item" id="facial_landmark_localization">-->
<!--			<table>-->
<!--				<tbody>-->
<!--					<tr><td class="project_image"><img src="images\FacialLandmarkLocalization.jpg" class="align-center" alt="Facial Landmark Localization" width="784" height="222" /></td></tr>-->
<!--					<tr><td class="project_title">Facial Landmark Localization</td></tr>-->
<!--					<tr><td class="project_misc">Industry Product @ CyberLink</td></tr>-->
<!--					<tr><td class="project_description">-->
<!--                        We train a facial landmark detector in an iterative optimization process by selecting the most discriminative feature in each iteration.-->
<!--                        The detection is real-time and robust to occlusions and hard shadows.-->
<!--                        Building upon the technique, CyberLink starts a startup, <a href="https://www.perfectcorp.com/business" class="red_link">Perfect Corp.</a>, that focuses on virtual makeup and <a href="https://www.businesswire.com/news/home/20221031005599/en/Perfect-Corp.-Debuts-on-the-New-York-Stock-Exchange-NYSE" class="red_link">goes public on the NYSE</a> in 2022-->
<!--                    </td></tr>-->
<!--				</tbody>-->
<!--			</table>-->
<!--			<div class="back_to_top"><a href="#navigator" class="black_link">back to top</a></div>-->
<!--		</div>-->

<!--		&lt;!&ndash; Face Login &ndash;&gt;-->
<!--		<div class="project_item" id="face_login">-->
<!--			<table>-->
<!--				<tbody>-->
<!--					<tr><td class="project_image"><img src="images\FaceLogin.jpg" class="align-center" alt="Face Login" width="784" height="229" /></td></tr>-->
<!--					<tr><td class="project_title">Face Login</td></tr>-->
<!--					<tr><td class="project_misc">Industry Product @ CyberLink</td></tr>-->
<!--					<tr><td class="project_description">-->
<!--                        Face Login takes as input video frames from an RGB webcam and recognizes human faces to determine authorized users.-->
<!--                        The system adopts online learning approach to update face models, hence robust to different light conditions.-->
<!--                    </td></tr>-->
<!--				</tbody>-->
<!--			</table>-->
<!--			<div class="back_to_top"><a href="#navigator" class="black_link">back to top</a></div>-->
<!--		</div>-->

<!--		&lt;!&ndash; FaceME &ndash;&gt;-->
<!--		<div class="project_item" id="face_me">-->
<!--			<table>-->
<!--				<tbody>-->
<!--					<tr><td class="project_image"><img src="images\FaceME.jpg" class="align-center" alt="FaceME" width="784" height="635" /></td></tr>-->
<!--					<tr><td class="project_title">FaceME &dash; (Face Clustering/Recognition in Photos/Videos)</td></tr>-->
<!--					<tr><td class="project_misc">Industry Product @ CyberLink</td></tr>-->
<!--					<tr><td class="project_description">-->
<!--                        FaceME recognizes and clusters faces in photos and videos with similar appearances, assisting users to effectively tag faces and organize personal datasets.-->
<!--                    </td></tr>-->
<!--				</tbody>-->
<!--			</table>-->
<!--			<div class="back_to_top"><a href="#navigator" class="black_link">back to top</a></div>-->
<!--		</div>-->

<!--		&lt;!&ndash; Chaplin &ndash;&gt;-->
<!--		<div class="project_item" id="chaplin">-->
<!--			<table>-->
<!--				<tbody>-->
<!--					<tr><td class="project_image"><img src="images\Chaplin.jpg" class="align-center" alt="Chaplin" width="784" height="274" /></td></tr>-->
<!--					<tr><td class="project_title">Chaplin &dash; (Hand Tracking Based Button Control System)</td></tr>-->
<!--					<tr><td class="project_misc">Industry Product @ CyberLink</td></tr>-->
<!--					<tr><td class="project_description">-->
<!--                        Chaplin tracks your hand movements to control the buttons displayed on the screen.-->
<!--                        Just wave your hand to activate the system, and it follows your hand's every move.-->
<!--                        All it takes is a webcam to provide the RGB frames as input.-->
<!--                    </td></tr>-->
<!--				</tbody>-->
<!--			</table>-->
<!--			<div class="back_to_top"><a href="#navigator" class="black_link">back to top</a></div>-->
<!--		</div>-->
<!--	<div>-->

<!--	<div class = "item_block" id="patents">-->
<!--		<div class="item_headline">-->
<!--			<h2>Patents </h2>-->
<!--		</div>-->

<!--		<div class="cv_item_content" id="patent_content">-->
<!--			<div class="patent_item">-->
<!--				<table>-->
<!--					<thead class="patent_table_head">-->
<!--						<tr>-->
<!--							<th scope="col" class="patent_number_head">Patent No.</th>-->
<!--							<th scope="col" class="patent_country_head">Country</th>-->
<!--							<th scope="col" class="patent_title_head">Title</th>-->
<!--						</tr>-->
<!--					</thead>-->
<!--					<tbody>-->
<!--						<tr>-->
<!--							<td class="patent_number"><a href="https://www.google.com/patents/US8121358">US 8,121,358</a></td>-->
<!--							<td class="patent_country">US</td>-->
<!--							<td class="patent_title"><b>C.-Y. Weng</b>, W.-T. Tsai, and C.-M. Lee, &quot;Method of Grouping Images by Faces&quot;</td>-->
<!--						</tr>-->
<!--						<tr>-->
<!--							<td class="patent_number"><a href="https://www.google.com/patents/US8531478">US 8,531,478</a></td>-->
<!--							<td class="patent_country">US</td>-->
<!--							<td class="patent_title">C.-M. Lee, <b>C.-Y. Weng</b>, &quot;Method of Browsing Photos based on People&quot;</td>-->
<!--						</tr>-->
<!--						<tr>-->
<!--							<td class="patent_number"><a href="https://www.google.com/patents/US8649602">US 8,649,602</a></td>-->
<!--							<td class="patent_country">US</td>-->
<!--							<td class="patent_title">S.-M. Tang, <b>C.-Y. Weng</b>, and J.-H. Huang, &quot;Systems and Methods for Tagging Photos&quot;</td>-->
<!--						</tr>-->
<!--						<tr>-->
<!--							<td class="patent_number"><a href="https://www.google.com/patents/US8693739">US 8,693,739</a></td>-->
<!--							<td class="patent_country">US</td>-->
<!--							<td class="patent_title"><b>C.-Y. Weng</b>, S.-M. Tang, and H.-C. Huang , &quot;Systems and Methods for Performing Facial Detection&quot;</td>-->
<!--						</tr>-->
<!--						<tr>-->
<!--							<td class="patent_number"><a href="https://www.google.com/patents/US8761519">US 8,761,519</a></td>-->
<!--							<td class="patent_country">US</td>-->
<!--							<td class="patent_title">M.-H. Chang, <b>C.-Y. Weng</b>, &quot;System and method for Selecting an Object Boundary in an Image&quot;</td>-->
<!--						</tr>-->
<!--						<tr>-->
<!--							<td class="patent_number"><a href="https://www.google.com/patents/US8769409">US 8,769,409</a></td>-->
<!--							<td class="patent_country">US</td>-->
<!--							<td class="patent_title"><b>C.-Y. Weng</b>, H.-C. Huang , &quot;Systems and Methods for Improving Object Detection&quot;</td>-->
<!--						</tr>-->
<!--						<tr>-->
<!--							<td class="patent_number"><a href="https://www.google.com/patents/US8867789">US 8,867,789</a></td>-->
<!--							<td class="patent_country">US</td>-->
<!--							<td class="patent_title">H.-W. Hsiao, <b>C.-Y. Weng</b>, &quot;Systems and Methods for Tracking an Object in a Video&quot;</td>-->
<!--						</tr>-->
<!--						<tr>-->
<!--							<td class="patent_number"><a href="https://www.google.com/patents/US9336583">US 9,336,583</a></td>-->
<!--							<td class="patent_country">US</td>-->
<!--							<td class="patent_title">H.-C. Huang, H.-W. Hsiao, <b>C.-Y. Weng</b>, and C.-D. Chung, &quot;Systems and Methods for Image Editing &quot;</td>-->
<!--						</tr>-->
<!--						<tr>-->
<!--							<td class="patent_number"><a href="https://worldwide.espacenet.com/publicationDetails/originalDocument?FT=D&date=20120425&DB=worldwide.espacenet.com&locale=en_EP&CC=JP&NR=4925370B2&KC=B2&ND=5">JP 4,925,370</a></td>-->
<!--							<td class="patent_country">JP</td>-->
<!--							<td class="patent_title"><b>C.-Y. Weng</b>, W.-T. Tsai, and C.-M. Lee, &quot;Method of Grouping Images by Faces&quot;</td>-->
<!--						</tr>-->
<!--					</tbody>-->
<!--				</table>-->
<!--			</div>-->
<!--		</div>-->
<!--	</div>-->

</div>
</body>

</html>
